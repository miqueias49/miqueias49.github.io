AGENTE DE APRENDIZADO POR REFORÇO TABULAR PARA NEGOCIAÇÃO DE AÇÕES

Dissertação apresentada ao Programa de Pós-Graduação em Ciência da Computação do Instituto de Ciências Exatas da Universidade Federal de Minas Gerais como requisito parcial para a obtenção do grau de Mestre em Ciência da Computação.

Resumo
Modelos de aprendizado supervisionado aplicados no contexto de negociação de ativos financeiros têm sido propostos e estudados há mais de duas décadas. Embora tenham alcançado bons resultados em termos de rendimento financeiro e risco, essa abordagem padece de limitações importantes, tais como a necessidade de retreinamentos constantes sobretudo nas grandes oscilações do mercado, além da dificuldade em converter um modelo com boa taxa de acertos nas previsões em um sistema de negociação que gere altos rendimentos. Essas limitações podem ser contornadas com a utilização de técnicas de Aprendizado por Reforço. Nessa abordagem, um agente pode aprender a negociar ativos financeiros para maximizar o ganho total ou minimizar o risco através de sua própria interação com o mercado. Além disso, também é capaz de manter-se atualizado a cada modificação do ambiente dispensando a necessidade de retreinamento uma vez que o agente está sempre em aprendizado. Para obter evidências dessas propriedades, desenvolveu-se um agente de aprendizado por reforço utilizando uma modelagem tabular com o algoritmo SARSA e aplicou-se esse agente em um conjunto de ações com variados padrões de tendência com o objetivo de observar como esse agente muda sua estratégia de negociação em cada situação de tendência. Além disso, desenvolveu-se um agente de negociação baseado em aprendizado supervisionado utilizando uma rede neural LSTM para comparar o seu desempenho com o do agente de aprendizado por reforço proposto. Aplicou-se ambos os agentes em um conjunto de 10 ações da Bovespa no ano de 2018, comparando métricas de rendimento financeiro, risco e taxas de acertos. Os resultados experimentais apresentaram evidências não só das limitações do agente de aprendizado supervisionado proposto, como também das aludidas propriedades do agente de aprendizado por reforço em se adaptar às mudanças no mercado de modo a produzir ganhos financeiros com menores perdas financeiras acumuladas.


Introdução
O sucesso de uma estratégia de investimento no mercado de ações negociadas em bolsas de valores é determinado pela sequência de decisões tomadas pelo investidor. Por isso, os investidores estão sempre atentos às cotações de preços, notícias, cenário político e econômico com o objetivo de detectar padrões que os permitam tomar as melhores decisões em cada situação.
Sabe-se que atualmente grande parte das negociações que ocorrem nas principais bolsas de valores do mundo são executadas por sistemas automatizados de negociação também chamados de robôs ou agentes de negociação.
Esses sistemas utilizam técnicas de Inteligência Artificial para detectar padrões ocultos em tempo real a partir de dados de preços, volume, notícias e outras informações. Uma vez detectado um determinado padrão pelo agente, o sistema executa automaticamente a operação mais adequada para os objetivos do investimento naquele momento.
Esses sistemas geralmente operam alta frequência e disputam por negócios que ofereçam ganhos muito pequenos. Porém, se essas oportunidades de negócios forem numerosas o suficiente eles podem acumular muitos ganhos. Uma vantagem desses sistemas é que eles podem detectar padrões e executar ordens em frações de segundo possibilitando ao investidor um melhor aproveitamento das oportunidades de negócios no mercado de bolsa de valores.
Uma das abordagens em Inteligência Artificial utilizadas é o Aprendizado por Reforço. Nessa abordagem um agente é capaz de aprender a associar ações a situações através da sua própria interação com o ambiente de modo a maximizar uma medida de desempenho em uma determinada tarefa.
Isto é semelhante a maneira como humanos aprendem várias tarefas durante a vida. Por exemplo, uma criança aprende a andar de bicicleta, jogar futebol ou andar de skate interagindo com esses objetos. É através da interação com esses objetos que uma criança aprende a relação entre uma ação tomada e a consequência da respectiva ação em cada situação. Desse modo, por tentativa e erro a criança adquire conhecimento e experiência e permanece sempre melhorando seu desempenho em cada tarefa.
Sistemas que empregam aprendizado por reforço têm obtido sucesso em diversas aplicações tais como jogos RTS, roteamento de veículos autônomos, jogos de Atari, jogos de tabuleiro e até aceleração de descobrimento de medicamentos.
No contexto de negociação de ações um sistema baseado em aprendizado por reforço é capaz de aprender através da sua própria interação com o mercado a associar a melhor decisão a cada estado de modo a otimizar uma medida de desempenho. Além disso, esse tipo de sistema possui uma importante característica adaptativa: o agente de aprendizado por reforço é capaz de modificar o que aprendeu anteriormente de forma dinâmica à medida que as condições do mercado se modificam.
Essas propriedades sugerem que um sistema de negociação baseado em aprendizado por reforço pode ser uma alternativa competitiva em termos de rendimento financeiro e risco perante outros tipos de modelagens de sistemas de negociação baseados em modelos de aprendizado supervisionado ou indicadores de Análise Técnica, por exemplo.
Portanto, pretende-se nesse trabalho de dissertação de mestrado conceber, propor, desenvolver e testar um agente de aprendizado por reforço para negociação de ações. Pretende-se ainda compará-lo com outro sistema baseado em aprendizado supervisionado. Para tanto, serão consideradas métricas de rendimento financeiro, risco e taxas de acertos.
Ao final, pretende-se obter evidências experimentais das propriedades adaptativas de um sistema de aprendizado por reforço aplicado ao contexto de negociação de ações bem como evidências de sua superioridade em termos de rendimento financeiro e risco comparado a um sistema baseado em aprendizado supervisionado.

Objetivos
O objetivo geral do presente trabalho é propor, desenvolver e analisar em termos de viabilidade técnica e financeira um sistema de negociação de ações utilizando aprendizado por reforço.
Objetivos Específicos
Para alcançar o referido objetivo geral, propõe-se os seguintes objetivos específicos:
Coletar dados históricos de ações da Bolsa de Valores de São Paulo (B3).
Identificar padrões de tendência nas séries temporais consideradas.
Implementar um agente de negociação utilizando um algoritmo de aprendizado por reforço.
Treinar e simular o agente em dados históricos das ações.
Analisar os resultados produzidos comparando com estratégias comuns e modelagens que empregam aprendizado supervisionado.

Contribuições
Espera-se ao final desse trabalho obter evidências experimentais que possam sugerir a viabilidade técnica e financeira do agente de aprendizado por reforço proposto. Além disso, outras contribuições importantes são:
Comparação do sistema de aprendizado por reforço implementado com um agente de negociação baseado em aprendizado supervisionado (e.g., redes neurais LSTM).
Análise do sistema implementado em diferentes condições de tendências de uma ação.
Análise do sistema implementado em um contexto de instabilidade do mercado de ações.

Organização do Trabalho
Esta dissertação está organizada da seguinte forma: além do presente Capítulo, no Capítulo 2 apresenta-se os conceitos fundamentais do mercado de ações para compreensão do restante do trabalho como também os conceitos básicos de aprendizado supervisionado e aprendizado por reforço. Em seguida, no Capítulo 3 os principais estudos empregando aprendizado por reforço em finanças são elencados destacando-se a divisão entre aprendizado por reforço e o aprendizado por reforço profundo bem como a necessidade de se estudar e comparar modelagens de aprendizado por reforço chamadas tabulares com as modelagens mais recentes. No Capítulo 4 é apresentada a proposta de modelagem do agente de aprendizado por reforço para negociação de ações e suas propriedades. Adiante, no Capítulo 5 são apresentados os objetivos da metodologia, suas fases, dados utilizados, as hipóteses subjacentes aos experimentos propostos bem como a modelagem do agente de aprendizado supervisionado utilizado como baseline na comparação com o agente de aprendizado por reforço proposto. No Capítulo 6 são mostrados os resultados de cada experimento e as respectivas análises tendo em vista as hipóteses e resultados esperados levantados no capítulo anterior. No Capítulo 7 é realizada uma síntese do trabalho e apresentados os resultados obtidos à luz do objetivo geral e das contribuições esperadas mencionadas no Capítulo 1. Além disso, são apresentados o escopo do trabalho e suas limitações bem como propostas de modelagens e abordagens para aprofundamento da utilização de técnicas de Aprendizado por Reforço no contexto de negociação de ativos financeiros como trabalhos futuros.

CAPÍTULO 2
Fundamentação Teórica
Neste Capítulo, são apresentados os conceitos básicos de mercados de ações, métricas de rendimento financeiro e risco que são essenciais para compreensão do restante do trabalho. São apresentados também os conceitos básicos de aprendizado supervisionado e aprendizado por reforço no contexto de mercado financeiro fundamentais para realização desse trabalho.

Mercado de Ações
O mercado de ações negociadas em bolsas de valores é um dos pilares no desenvolvimento de economias capitalistas modernas. É através do mercado de ações, pela venda direta de participação no seu patrimônio líquido representado pelas ações, que as sociedades anônimas captam os recursos necessários ao seu desenvolvimento negocial e patrimonial, assumindo o compromisso de remunerar os seus acionistas em função do capital nela aplicado e de seus resultados futuros.
É também através do mercado de ações e outros títulos financeiros negociados em bolsa que investidores tem a oportunidade de auferir rendimentos acima de outras aplicações menos arriscadas através de livre negociação especulativa de ativos financeiros. Naturalmente, os preços das ações, como qualquer bem livremente negociado, é determinado pela lei de oferta e procura. Por conseguinte, esses preços são afetados por diversos fatores tais como as expectativas dos investidores em relação a empresa, as condições econômicas e políticas do país, taxas de juros, inflação, câmbio etc.
A variação relativa dos preços de uma ação entre os instantes t − 1 e t é chamada taxa de retorno e é definida por (Equação 2.1):

RT igual, p de te menos p de t menos um, dividido por, p de t menos um.
onde rt denota a taxa de retorno no tempo t, Pt e Pt−1 denotam respectivamente os preços da ação nos tempos t e t − 1.
A variação média dos retornos ao longo tempo é geralmente chamada de volatilidade e pode ser medida de diversas formas sendo a mais comum o desvio padrão dos retornos. Momentos de incerteza e indefinições no mercado costumam apresentar alta volatilidade o que dificulta a tarefa de prever qual o próximo movimento da série de preços de uma ação para identificar uma tendência. Investidores buscam detectar o início ou final de uma tendência para tomar uma decisão de comprar ou vender um ativo. Em geral, os investidores compram uma ação no início de uma tendência de alta por um preço baixo e vendem a ação quando a tendência termina e começa a cair por um preço mais auferindo o respectivo rendimento.
Porém, detectar quando começa e quando termina uma tendência não é uma tarefa simples. Por isso, costuma-se utilizar indicadores de Análise Técnica buscando caracterizar e identificar tendências ou reversões de tendências. Embora, os indicadores de Análise Técnica sejam bastante utilizados por investidores e analistas do mercado, nem sempre eles conseguem prever precisamente o início ou o fim de uma tendência. Apesar disso, uma vez detectado ou previsto o início de uma tendência de alta ou de baixa o investidor pode iniciar uma posição buscando obter ganhos em ambas as situações.
Diz-se que o investidor está posicionado em uma posição comprada quando o investidor compra uma ação esperando vendê-la futuramente por preço maior que quando comprou. O retorno r long de uma posição long pode ser calculado como (Equação 2.2):

R long igual, p venda + D. menos p compra. dividido por, p compra.

sendo P venda o preço posterior pelo qual o investidor vendeu o ativo, D os dividendos distribuídos durante a posição comprada e P compra o preço anterior pelo qual o investidor comprou o ativo.
É também possível aproveitar as tendências de baixa para obter ganhos. Considerando que uma ação está em tendência de baixa e que o investidor não possui essa ação no momento, ele pode tomar emprestado essa ação de outro investidor por meio de uma corretora e vendê-la em seguida esperando recomprá-la futuramente por um preço ainda menor para então devolvê-la ao seu titular original e embolsar a diferença dos preços. Essa operação é chamada venda a descoberto ou shorting em inglês. O investidor inicia uma posição vendida, a descoberto ou short vendendo uma ação e termina essa posição recomprando essa ação e devolvendo-a ao seu titular. O retorno r short de uma posição vendida é dado por (Equação 2.3):
em que P compra é o preço posterior pelo qual o investidor recomprou a ação, D os dividendos distribuídos durante a posição vendida e P venda o preço anterior pelo qual o investidor vendeu a ação e que iniciou a posição vendida.
Contudo, a posição vendida apresenta um risco maior comparado ao risco de uma posição comprada. Em uma posição vendida, caso o preço da ação suba em uma tendência de alta o investidor deverá recomprá-la para devolvê-la ao seu titular original por um preço arbitrariamente maior do que preço de quando vendeu a ação para iniciar o shorting. E como o preço da ação pode subir infinitamente, o prejuízo de uma posição vendida também pode ser muito grande a tal ponto do investidor necessitar de retirar do próprio patrimônio para recomprar a ação e devolvê-la a seu titular original o que pode levá-lo à insolvência. 
Para evitar que isso ocorra, as bolsas ou as corretoras costumam exigir do investidor uma garantia ao iniciar uma posição vendida. Essa garantia, que pode ser dada em dinheiro, é chamada margem de garantia e serve para resguardar o investidor de uma perda significativa decorrente de uma posição vendida. As corretoras costumam terminar automaticamente uma posição vendida caso o prejuízo alcance um determinado valor da margem de garantia.
Já na posição comprada o risco decorre do fato do preço da ação cair. Nesse caso, o prejuízo ficará restrito ao valor que o investidor gastou ao comprar a ação uma vez que o preço de uma ação não pode ser nulo.
Para limitar as perdas costuma-se utilizar as chamadas travas ou stops. Uma trava de perda ou stop-loss é estabelecida ao enviar uma ordem de compra para iniciar uma posição long ou ao enviar uma ordem de venda para iniciar uma posição short. Caso a perda em relação o início da posição ultrapasse o valor estabelecido no stop-loss a posição é automaticamente terminada comprando-se ou vendendo-se o ativo pelo preço atual.
O investidor pode também limitar os ganhos já obtidos para se resguardar de uma eventual perda que venha a reduzir os ganhos já alcançados. Isso é feito através uma trava de ganho ou take-profit que é estabelecida ao iniciar uma posição comprada ou vendida. Uma vez alcançado o nível de lucro na posição igual ou superior ao estabelecido no take-profit a posição é automaticamente encerrada pelo preço atual da ação.
O desempenho de um investimento ou estratégia pode ser medido de várias maneiras. A principal métrica é o rendimento financeiro total ou final R total que pode ser calculado pela diferença entre o valor inicial investido V inicial e o valor final do montante alcançado V final (Equação 2.4):
Outra medida de desempenho é o máximo drawdown (Equação 2.5). O máximo drawdown MDD é a maior perda cumulativa a partir de um pico de capital alcançado até um vale posterior no tempo ao referido pico. Se o máximo drawdown de uma estratégia for muito elevado comparado com outras opções de investimento isso pode indicar que a estratégia não é adequada para a ação. O máximo drawdown é calculado por (Equação 2.5):
em que C t denota o valor do capital acumulado em um pico e C t denota o capital acumulado no vale e t é o tempo no final da estratégia. A maior das diferenças C t - C t é o máximo drawdown.
Investidores costumam preferir dentre várias estratégias de investimento aquela que proporcione o menor risco ou variabilidade. Assim, uma importante medida de comparação entre estratégias é o chamado Índice Sharpe. Considerando uma estratégia de investimento, seja r p o retorno total dessa estratégia e σ p o desvio padrão dos retornos ao longo dessa estratégia. Seja ainda r f o retorno de uma outra estratégia chamada livre-de-risco como por exemplo um título do tesouro. O índice Sharpe é calculado como (Equação 2.6):
Assim, quanto maior o índice Sharpe melhor é o desempenho combinado entre risco e retorno de uma estratégia. O índice Sharpe pode ser interpretado como o retorno ou recompensa em excesso por unidade de risco por utilizar o investidor a estratégia considerada e não o investimento da taxa livre-de-risco. É a medida da "recompensa" que o investidor ganha por adotar a estratégia mais arriscada que a da taxa livre-de-risco.
O índice Sharpe é obtido através do coeficiente angular da chamada linha de alocação de capital definida pela Equação 2.7.
onde E( r c ) é o retorno esperado de um portfólio c contendo um ativo livre de risco com taxa de retorno r f e um ativo de risco com retorno esperado denotado por E( r p ) e desvio padrão σ p. O termo σ c denota o desvio padrão dos retornos do portfólio.
O conjunto de todas combinações possíveis de portfólios da carteira c com retornos esperados E(rc) e desvios padrões σc é delimitado por uma curva chamada fronteira eficiente em um gráfico (Vide gráfico da Figura 2.1) onde o eixo x contém os valores de risco σc e o eixo y contém os valores de retornos esperados E(rc) da carteira c. O ponto da linha de alocação de capital que toca a curva da fronteira eficiente contém o portfólio possível com maior retorno esperado e menor risco.
Se a série de retornos de uma estratégia apresenta retornos positivos significativos isso pode aumentar o denominador na Equação 2.6 penalizando os retornos positivos e diminuindo o valor do índice Sharpe fornecendo, portanto, uma falsa impressão a respeito da relação risco-retorno da estratégia.
Para contornar essa limitação do índice Sharpe costuma-se utilizar o chamado índice Sortino. Nesse caso, considera-se apenas os retornos não positivos da série de retornos da estratégia para o cálculo do desvio padrão. O índice Sortino é calculado pela equação (Equação 2.8):
Sortino ratio = r p - r d, dividido por, desvio padrão dos retornos não positivos.

Em que RP é o retorno devido à estratégia de investimento e RD eu chamado um mínimo retorno aceitável. O termo no denominador é o desvio padrão dos retornos não positivos que pode ser calculado através da raiz quadrada da semi variância (equação 2. 9):

Onde n é o número total de retornos ao longo da estratégia ei RI é o retorno obtido no tempo i.

Aprendizado supervisionado
Um modelo de aprendizado supervisionado relacionar as respostas as instâncias de um problema com o objetivo de prever as respostas em futuras observações ou entender melhor o relacionamento entre as respostas e as entradas. Cada instância é composta por uma ou mais variáveis chamadas Pretores ou atributos.
Uma vez treinado, a capacidade do modelo de prever corretamente as respostas em instâncias que não estão presentes nos dados de treinamento é chamada de generalização. Para alcançar boa capacidade de generalização esses modelos tipicamente necessitam de grandes volumes de dados para treinamento.
Dentre os modelos de aprendizado supervisionado utilizados em sistemas de negociação de ativos financeiros estão as redes neurais MLP, svm, árvores de decisão, redes de aprendizado profundo, redes neurais LSTM. 
Nesse tipo de aplicação, esses modelos costumam utilizar como dados de treinamento as séries de preços e volumes em diversas periodicidades, dados de negociações, posições no livro de oferta, dados de análise de sentimento, indicadores de análise técnica, ETC.
Pode-se ainda utilizar dados de análise fundamentalista os quais buscam determinar o preço adequado para uma ação com base na análise de situação financeira de uma empresa. Caso esse preço ultrapasse o valor atual da ação análise fundamentalista pode indicar uma oportunidade de compra dessa ação. Contudo, dados e indicadores de análise fundamentalista são calculados com base no Balanço das empresas e são divulgadas com pouca frequência o que torna a utilização desses 2 inadequada para operações de negociações realizadas durante o dia. Por isso, utilizou-se neste trabalho somente dados de análise técnica os quais podem ser calculados diretamente à partir dos preços das ações em qualquer periodicidade.
Uma vez treinados, esses modelos podem ser combinados com estratégias de negociação pré estabelecidas. Essas estratégias geralmente são baseadas em indicadores de análise técnica e provém tanto da experiência do próprio investidor além da experiência de outros investidores.
Combinando sim modelos de aprendizado supervisionado e indicadores de análise técnica obtém-se em uma estratégia de operação que produz sinais representando ordens de operação no mercado as quais constituem a estratégia de operação do robô. 
Contudo, esse tipo de modelagem de sistema de negociação apresenta algumas limitações. A primeira delas advém da estratégia baseada em indicadores de análise técnica. A literatura de análise técnica; apresenta diversos indicadores técnicos divididos em várias categorias como indicadores de tendência, volatilidade, volume, momento, osciladores, ETC. Selecionar quais indicadores usar em uma estratégia assim como quais valores de parâmetros para cada indicador simples. Embora possa sim utilizar algoritmos evolucionários para selecionar e ajustar indicadores, essa abordagem fica ainda limitada a condições momentânea dos ativos em que foi aplicado. Ademais, uma estratégia que é lucrativa em um determinado momento e para um determinado ativo financeiro pode não ser mais no instante seguinte devido às variações de tendência, volatilidade e liquidez aqui o ativo está sujeito.
Outra limitação se deve à necessidade de re treinamentos cada vez que o mercado apresenta condições significativamente distinta aquelas em que o robô foi treinado. Considerando-se que o mercado de ações pode estar sujeito a oscilações de volatilidade, tendência, volume e liquidez em intervalos tão curtos quanto um dia, 1 hora ou minutos, a necessidade de re treinamentos para se adaptar a tais mudanças momentâneas do mercado pode implicar na perda de oportunidade de negócio para o investidor. Isso é ainda mais claro quando se atenta ao fato de que um ciclo de re treinamento implica em treinar o robô em dados passados, validar o modelo gerado, testar via back test, otimizar parâmetros simular em dados em tempo real para só então colocar o sistema em produção. Se todo esse ciclo for suficientemente longo as perdas de oportunidades podem até mesmo inviabilizar a utilização do sistema.
Outro fato importante é que o modelo de aprendizado supervisionado otimizam funções objetivo diferentes ou não relacionadas as funções objetivos do sistema de negociação em que o modelo será aplicado o que também constitui outra limitação. Um modelo de aprendizado supervisionado que apresente uma taxa de acerto não necessariamente implica que o sistema de negociação em que será utilizado gerará altos rendimentos financeiros porque as perdas financeiras obtidas podem superar os ganhos quando o modelo acerta.
São essas limitações que sugerem a concepção, o estudo e a experimentação de sistemas de negociação baseados em aprendizado por reforço.

Aprendizado por reforço
Segundo Russell & Norvig, é possível considerar que o aprendizado por reforço abrange todos os elementos da inteligência artificial: um agente é colocado em um ambiente e deve aprender a agir satisfatoriamente nesse ambiente daí para frente. Ou seja, no aprendizado por reforço um agente deve aprender a executar uma tarefa em um ambiente através da sua própria experiência nesse ambiente. Para Szepesvári, o aprendizado por reforço enquanto o problema de aprendizado, consiste em aprender a controlar um sistema para maximizar algum valor numérico que representa um objetivo no longo prazo.
Enquanto no aprendizado supervisionado um modelo recebe um conjunto de dados de treinamento que associa instâncias de um problema as respostas corretas, no aprendizado por reforço o agente não dispõem desse tipo de dado de treinamento e, portanto, deve aprender através de sua própria interação com um ambiente a associar as ações corretas aos estados do ambiente.
Essa interação consiste essencialmente em escolher e executar uma ação em cada estado do ambiente e avaliar a consequência da ação tomada naquele estado. Ao executar uma ação, o ambiente responde ao agente no instante seguinte mudando de estado e enviando uma resposta numérica chamada recompensa ou reforço. É por meio do valor desse reforço que o agente pode avaliar se a ação executada naquele estado foi boa ou ruim e com isso pode aprender a melhor ação em cada estado do ambiente, razão pela qual esse tipo de aprendizado é chamado de aprendizado por reforço.
Considerando o acúmulo de recompensas como a medida de desempenho do agente, ao maximizar essa medida ou a gente pode executar a tarefa de maneira ótima. Portanto, o objetivo do agente de aprendizado por reforço é este mar as ações que maximizam a soma de recompensas no longo prazo. Essas ações constituem a chamada política ótima do agente.
Em geral, um problema de aprendizado por reforço pode ser descrito formalmente como um processo de decisão de markov. Considerando uma sequência discreta de tempo, um MDP pode ser definido por:
Um conjunto finito s de estados sendo ST e ST + 1 variáveis aleatórias que denota um estado ST, ST mais um pertencentes à s nos instantes t e t mais um.
Um conjunto finito a de ações sendo um ST em função de a contém a o conjunto de ações possíveis em cada estado ST e uma variável aleatória de notando a ação a de t pertencente a ST em função de a executada no estado ST. 
Um conjunto finito de recompensas R contido no conjuntos reais sendo r de t mais um uma variável aleatória denotando a recompensa r tem mais um pertencente ao conjunto r obtida no instante t mais um.
uma função recompensa ST e a t em função de r igual
O comportamento de um agente de aprendizado por reforço pode ser descrito da seguinte forma seja um espaço de estados qualquer, por exemplo, modelo s. Seja também um espaço de ações para qualquer estado dado por, por exemplo. Em um estado num instante de tempo o agente escolhe uma ação de maior valor para respectivo estado na tabela, isto é, a que apresenta o maior valor esperado de soma de recompensas no longo prazo.
Escolhida a ação caceta, o agente executa essa ação fazendo com que o ambiente mude para o estado caceta e retorne para o agente uma recompensa caceta. A gente, em tão, atualiza sua experiência a respeito da consequência de tomar uma ação caceta no estado caceta e receber a recompensa caceta. Em seguida, prossegue escolhendo uma nova ação caceta a ser executada no estado caceta e segue dessa forma a cada novo estado até encontrar o estado final. 
O conhecimento adquirido pelo através de sua experiência é modelado na forma de uma função chamada função valor. Um função valor pode ser uma função de estado valor Casseta ou uma função de estado ação Casseta que, por exemplo, pode ser modelada como uma tabela tal como na figura 2. 3.
O valor da função de resultado de valor Casseta indica o valor esperado das recompensas acumuladas que um agente pode obter se começar pelo estado caceta. Por sua vez, valor da função estado ação caceta para cada par caceta em se da o valor esperado de recompensas acumuladas que um agente pode obter de começar o estado Casseta e tomar a ação caceta. O processo de estimação dessa função utiliza métodos de programação dinâmica baseados na equação de otimalidade de bellman.
Após cada instante periquito o agente recebe uma sequência de recompensas periquito nos instantes seguintes cuja a soma periquito é chamada de retorno. Se a tarefa em questão possui um estado final definido então a sequência ti tempo periquito é finita e assim também o retorno periquito. Porém, se a tarefa for contínua e, Isto É, do tipo que não possui um estado final definido, a sequência de tempo será infinita como também o retorno. Para lidar com esse problema utiliza-se ou conceito de desconto. Nessa abordagem, o agente seleciona ações de modo que as somas das recompensas descontadas no futuro sejam maximizadas. Ou a gente então busca selecionar periquito que possa maximizar o retorno descontado.
Por isso, o fator periquito na equação periquito é chamada taxa de desconto e permite ponderar a importância dos retornos imediatos em relação às recompensas futuras. O objetivo desse fator é fazer com que o somatório de recompensas seja um valor finito. Assim, quanto mais próximo de zero for o fator periquito, maior será o peso dado às recompensas imediatas mais próximas do momento presente. Por outro lado, quanto mais próximo de um, maior será o peso das recompensas futuras.
Uma vez estimada a função-valor, o agente pode escolher as ações que maximizam periquito para cada estado que encontrar. Esse mapeamento de estados em ações que levam ao máximo acúmulo de recompensas é chamado de política ótima periquito. A política ótima pode ser extraída da função de estado-ação ótima periquito.
Para estimar essa função, o agente necessita experimentar todas as ações possíveis em cada estado para descobrir qual delas leva há estados de mais alto valor na função de estado-ação. Porém, se o agente sempre escolher a ação que maximiza periquito em todo estado periquito ele poderá deixar de conhecer estados de maior valor e que levam a um maior acúmulo de recompensas. Esse dilema chamado Exploration versus Exploitation, é comum em aprendizado por reforço e existem várias técnicas que buscam balancear esses 2 aspectos do aprendizado. Uma delas é chamada periquito e consiste em selecionar arbitrariamente com probabilidade pequena periquito uma ação periquito que não necessariamente maximiza a função de estado ação periquito em que um estado periquito ou então selecionar de maneira gulosa com probabilidade periquito a ação que maximiza a função de estado ação.
Os algoritmos de programação dinâmica periquito e periquito utilizam a equação de otimalidade de bellman para resolver um problema de aprendizado por reforço modelado como um MDP. Por isso um são chamados algoritmos baseados em modelo porque necessitam do modelo probabilístico da dinâmica de transição de estado e recompensas do ambiente.
Porém, na prática é muito raro se não impossível obter um modelo da dinâmica do ambiente. Por exemplo, é inviável este mar um modelo de transição de estados e recompensas de um determinado jogador de xadrez dada a enorme quantidade de possíveis no jogo outras particularidades do próprio jogo e do adversário. Por isso, utiliza se algoritmos chamados periquito que dispensam um modelo do ambiente.
Dentre os algoritmos periquito estão os algoritmos de diferença temporal dos quais sarça e periquito são exemplos. O que esses algoritmos fazem é observar a diferença entre a estimativa atual da função de estado a ação, o valor descontado da função de estado à ação para o próximo estado periquito e a recompensa obtida periquito para então corrigir a estimativa anterior. Assim, quando o agente no estado periquito escolhe e executa uma ação periquito, o ambiente muda para o estado um periquito e retorna a recompensa periquito com as quais a função d está doação é atualizada.
Essa técnica baseia-se na ideia de que como o valor da função de estado a ação periquito corresponde ao instante posterior, ela tem mais chance de estar correta. Esse valor pode ser descontado pelo fator de desconto periquito isso somado a recompensa obtida tornando-se o novo valor para A estimativa.
Os algoritmos periquito e periquito utilizam regras diferentes para atualização da função de estado ambos utilizam o conceito de diferença temporal. No algoritmo periquito a função de estado à ação é atualizada na forma da equação.
Onde o parâmetro periquito é chamado de taxa de aprendizado. O algoritmo periquito sua vez um utiliza uma regra de atualização diferente considerando o estado periquito, a ação então, a recompensa obtida periquito e o estado alcançado periquito e a próxima ação a ser executada periquito onde dá origem ao seu nome periquito. Enquanto os espaços estados de um problema de aprendizado por reforço forem pequenos e discreto as funções de valor podem ser modeladas na forma de uma tabela. Denomina se esse tipo de modelagem tabular. Porém, muitos problemas práticos apresentam espaços de estados ou ações contínuos e que inviabiliza a modelagem na forma tabular.
Como um espaço de estado tão grande o aprendizado só é possível por meio de a mostras disse espaço utilizando técnica de aproximação de funções que buscam generalizar a saídas da função para as demais instâncias do espaço de estados. Esse é um problema comum em aprendizado supervisionado ou qual oferece várias técnicas como as redes neurais, regressão linear de decisão redes neurais profundas e outros para Aproximações de funções.
Combinando as técnicas existentes de aproximação de fusões com os algoritmos de aprendizado por reforço pode-se estimar uma aproximação para as funções de valor em problemas com espaço de estados muito grandes.
Nesse caso, a função de estado valor periquito é parametrizada por um vetor de pesos periquito. Considerando um estado qualquer periquito como um vetor de dimensão periquito dado por periquito, uma formulação possível para a função de estado valor utiliza uma função linear nos pesos tal como uma equação periquito. Derivando a função periquito em relação a periquito pode-se obter uma regra de atualização que utiliza a técnica do gradiente descendente estocástico para estimar a função de valor.
